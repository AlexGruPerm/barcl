/*

Just small parts, queries and any for future spark application.
Work with Map Arrays in Spark
https://medium.com/@mrpowers/working-with-spark-arraytype-and-maptype-columns-4d85f3c8b2b3

ETL workflow
https://medium.com/@mrpowers/how-to-write-spark-etl-processes-df01b0c1bec9

*/

spark-shell --total-executor-cores 3 --driver-memory 600M --executor-memory 3G --num-executors 1 --executor-cores 2 --jars "/opt/spark-2.3.2/jars/spark-cassandra-connector-assembly-2.3.2.jar" --conf "spark.cassandra.connection.host=192.168.122.192"

import org.apache.spark.sql._
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector._
import spark.implicits._

//===================================================================================================

case class FormKeys(ticker_id     :Int,
                    bar_width_sec :Int)

case class Form(ticker_id     :Int,
                bar_width_sec :Int,
                ts_begin      :Long,
                ts_end        :Long,
                log_oe        :Double,
                res_type      :String,
                formdeepkoef  :Int,
                frmConfPeak   :Int)

case class ccLibSvm(label    :Int,
                    features :Int)


/**
*  without as[Form] getFormsDb: (TickerID: Int, BarWidthSec: Int)org.apache.spark.sql.DataFrame
*     with as[Form] getFormsDb: (TickerID: Int, BarWidthSec: Int)org.apache.spark.sql.Dataset[Form]
*/


 def getKeysFormsDb(BarWidthSec: Int) :Dataset[FormKeys] = {
            import org.apache.spark.sql.functions._
            spark.read.format("org.apache.spark.sql.cassandra")
              .options(Map("table" -> "bars_forms", "keyspace" -> "mts_bars"))
              .load()
              .where(col("bar_width_sec") === BarWidthSec)
              .select(
                col("ticker_id"),
                col("bar_width_sec")
                ).distinct.sort(asc("ticker_id"),asc("bar_width_sec")).as[FormKeys]}

val dsKeys = getKeysFormsDb(600);

dsKeys.show()

 def getFormsDb(TickerID :Int, BarWidthSec: Int) :Dataset[Form] = {
            import org.apache.spark.sql.functions._
            spark.read.format("org.apache.spark.sql.cassandra")
              .options(Map("table" -> "bars_forms", "keyspace" -> "mts_bars"))
              .load()
              .where(col("ticker_id") === TickerID)
              .where(col("bar_width_sec") === BarWidthSec)
              .select(
                col("ticker_id"),
                col("bar_width_sec"),
                col("ts_begin"),
                col("ts_end"),
                col("log_oe"),
                col("res_type"),
                col("formdeepkoef"),
                col("formprops")("frmConfPeak").as("frmConfPeak").cast("Int")
                ).sort(asc("ticker_id"),asc("bar_width_sec"),asc("ts_begin")).as[Form]}

val seqDsForms :Seq[Dataset[Form]] = dsKeys.collect.toSeq.map(elm => getFormsDb(elm.ticker_id,elm.bar_width_sec))
val ds :Dataset[Form] = seqDsForms.reduce(_ union _)
ds.cache()
ds.count()
spark.catalog.dropGlobalTempView("frms")
ds.createGlobalTempView("frms")

spark.sql("""
             select res_type,frmConfPeak
               from global_temp.frms
""").show()

/*
libsvm package implements Spark SQL data source API for loading LIBSVM data as DataFrame.
The loaded DataFrame has two columns:
label containing labels stored as doubles and
features containing feature vectors stored as Vectors.

label index1:value1 index2:value2
where the indices are one-based and in ascending order.

VectorAssembler is a transformer that combines a given list of columns into a single vector column.

spark.read.format("libsvm").option("numFeatures", "2").load("/root/sample_data.txt")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]  >> !!!!!!!!
*/

val mlsrc = ds.withColumn("label",
   when($"res_type" === "mx", 1).otherwise(
    when($"res_type" === "mn", 0).otherwise(2))).select(col("label"),col("frmConfPeak").as("features"))

import org.apache.spark.sql.types.{DoubleType}

val mlsrc = ds.withColumn("label",
   when($"res_type" === "mx", 1).otherwise(
    when($"res_type" === "mn", 0).otherwise(2))).
     select(
      col("label").cast(DoubleType),
      col("ticker_id"),
      col("frmConfPeak")
     )

import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors

val assembler = new VectorAssembler().setInputCols(Array("ticker_id", "frmConfPeak" )).setOutputCol("features")
val vd = assembler.transform(mlsrc).drop("ticker_id").drop("frmConfPeak")

>>>
like libsvm
vd: org.apache.spark.sql.DataFrame = [label: double, features: vector]
>>>

import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val data = vd
val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
val train = splits(0)
val test = splits(1)
println(" rows:"+data.count()+" train:"+train.count()+" test:"+test.count())

val layers = Array[Int](2, 5, 5, 10)
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setLabelCol("label").setFeaturesCol("features").setBlockSize(128).setSeed(1234L).setMaxIter(1000)
val model = trainer.fit(train) //we can save it for future use

val result = model.transform(test)
result.show()




/*

Convert dataset rowis into CC type
val dat = mlsrc.map(r => ccLibSvm(r.getAs[Int]("label"),r.getAs[Int]("features")))

import org.apache.spark.sql.types.IntegerType
val df2 = df.withColumn("yearTmp", df.year.cast(IntegerType))
    .drop("year")
    .withColumnRenamed("yearTmp", "year")

val assembler = new VectorAssembler().setInputCols(Array("TIR", "UO", "ROC20" )).setOutputCol("FEATURES")
val vd = assembler.transform(dat)

scala> mlsrc.printSchema()
root
 |-- label: decimal(38,18) (nullable = true)
 |-- features: integer (nullable = true)



//next we can use frms in Spark.sql

//#1 Query - count of forms by ticker
spark.sql("""
             select ticker_id,
                    count(*) as forms_cnt
               from global_temp.frms
               group by ticker_id
              order by 2 desc
""").show(50)

//#2 Query - count of rows by groups : frmConfPeak,res_type
spark.sql("""
             select frmConfPeak,res_type,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak,res_type
              order by 2,3 desc
""").show(50)

spark.sql("""
             select frmConfPeak,res_type,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak,res_type
              order by 1,2,3
""").show(50)

spark.sql("""
             select frmConfPeak,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak
              order by 2,1
""").show(50)

====================== CLUSTERING KMEANS ======================

//source data : metrics.


val ds_frm_1_600 = getFormsDb(1,600)
val ds_frm_3_600 = getFormsDb(3,600)

val dsFrms = ds_frm_1_600.union(ds_frm_3_600)

spark.catalog.dropGlobalTempView("fsrc")
dsFrms.createGlobalTempView("fsrc")

spark.sql("""
             select count(*) as cnt
               from global_temp.fsrc
""").show()

spark.sql("""
             select
                    ticker_id,
                    bar_width_sec,
                    ts_begin,
                    ts_end,
                    from_unixtime(ts_end/1000,'dd-MM-YYYY')  as ts_end_ddate,
                    (ts_end-ts_begin)/1000                   as ts_eb_secs
               from global_temp.fsrc
              order by ts_begin asc
""").show()

unuseful incorrect forms with small deep.

+---------+-------------+-------------+-------------+------------+----------------+
|ticker_id|bar_width_sec|     ts_begin|       ts_end|ts_end_ddate|      ts_eb_secs|
+---------+-------------+-------------+-------------+------------+----------------+
|        3|          600|            0|1549290516164|  04-02-2019|1.549290516164E9|
|        3|          600|1549290516164|1549290516164|  04-02-2019|             0.0|
|        3|          600|1549290516164|1549291103752|  04-02-2019|         587.588|
|        3|          600|1549290516189|1549291103752|  04-02-2019|         587.563|
|        3|          600|1549522692203|1549526292016|  07-02-2019|        3599.813|
|        3|          600|1549522692259|1549526292016|  07-02-2019|        3599.757|
|        3|          600|1549525692122|1549529291869|  07-02-2019|        3599.747|
|        3|          600|1549525692268|1549529291869|  07-02-2019|        3599.601|
|        3|          600|1549526292182|1549529892062|  07-02-2019|         3599.88|
|        3|          600|1549526293633|1549529892062|  07-02-2019|        3598.429|
|        3|          600|1549542492949|1549546092279|  07-02-2019|         3599.33|
|        3|          600|1549542495085|1549546092279|  07-02-2019|        3597.194|
|        1|          600|1549659600332|1549663200271|  08-02-2019|        3599.939|
|        3|          600|1549659600332|1549663200329|  08-02-2019|        3599.997|
|        1|          600|1549659601334|1549663200271|  08-02-2019|        3598.937|
|        3|          600|1549659601334|1549663200329|  08-02-2019|        3598.995|
|        3|          600|1549890473388|1549894073354|  11-02-2019|        3599.966|
|        3|          600|1549890473412|1549894073354|  11-02-2019|        3599.942|
|        1|          600|1549891112686|1549894712628|  11-02-2019|        3599.942|
|        1|          600|1549891113270|1549894712628|  11-02-2019|        3599.358|
+---------+-------------+-------------+-------------+------------+----------------+

//convert it into scala collection
val ora_parts_list = df_ora_parts_names.as[(String)].collect.toSeq

for (ora_part_name <- ora_parts_list){
 println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
 println("        Processing partition:   "+ora_part_name)
 println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

dsFrms.count()

df_forms_src.show()
df_forms_src.printSchema()
Dataset<Row> ds3 = ds1.union(ds2);


scala> df_forms_src.printSchema()
root
 |-- ticker_id: integer (nullable = true)
 |-- bar_width_sec: integer (nullable = true)
 |-- ts_begin: long (nullable = true)
 |-- ts_end: long (nullable = true)
 |-- log_oe: double (nullable = true)
 |-- res_type: string (nullable = true)
 |-- formdeepkoef: integer (nullable = true)
 |-- frmConfPeak: integer (nullable = true)

*/




























/*

DIFF

 val dat = df_tdata.filter(df_tdata("ddate") === "20180601").cache()
 val dat_norm_1 = dat.withColumn("svalTmp", dat("sval").cast(DoubleType)).drop("sval").withColumnRenamed("svalTmp","sval")


 case class rowData(ddate: Int,id_pok: Int,id_row: String, sval: BigDecimal)

 val df_tdata_ds = spark.read.format("org.apache.spark.sql.cassandra").options(Map("table"->"t_data","keyspace"->"msk_arm_lead")).load()
                   .filter($"ddate" === "20180601")
                   .withColumn("sval",$"sval".cast(DecimalType(38,18)))
                   .as[rowData]

val dat = spark.read.format("org.apache.spark.sql.cassandra").options(Map("table"->"t_data","keyspace"->"msk_arm_lead"))
.load().filter($"ddate" === "20180601").withColumn("sval",$"sval".cast(DecimalType(38,18))).as[rowData]

dat: org.apache.spark.sql.Dataset[rowData] = [ddate: int, id_pok: int ... 2 more fields]

case class rowNewData(ddate: Int,id_pok: Int,id_row: String, sval: BigDecimal, cntrlParam: Int)

val new_dat = dat.map { row =>(row: @unchecked) match {case rowData => rowNewData(row.ddate, row.id_pok, row.id_row, row.sval, {if (row.sval>65000.0) 1 else 0} )}}

*/