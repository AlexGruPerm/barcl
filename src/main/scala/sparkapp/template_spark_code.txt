/*

Just small parts, queries and any for future spark application.
Work with Map Arrays in Spark
https://medium.com/@mrpowers/working-with-spark-arraytype-and-maptype-columns-4d85f3c8b2b3

ETL workflow
https://medium.com/@mrpowers/how-to-write-spark-etl-processes-df01b0c1bec9

libsvm package implements Spark SQL data source API for loading LIBSVM data as DataFrame.
The loaded DataFrame has two columns:
label containing labels stored as doubles and
features containing feature vectors stored as Vectors.

label index1:value1 index2:value2
where the indices are one-based and in ascending order.

VectorAssembler is a transformer that combines a given list of columns into a single vector column.

spark.read.format("libsvm").option("numFeatures", "2").load("/root/sample_data.txt")
data: org.apache.spark.sql.DataFrame = [label: double, features: vector]  >> !!!!!!!!

like libsvm
vd: org.apache.spark.sql.DataFrame = [label: double, features: vector]


  without as[Form] getFormsDb: (TickerID: Int, BarWidthSec: Int)org.apache.spark.sql.DataFrame
     with as[Form] getFormsDb: (TickerID: Int, BarWidthSec: Int)org.apache.spark.sql.Dataset[Form]


import org.apache.spark.ml.feature.SQLTransformer

val df = spark.createDataFrame(
  Seq((0, 1.0, 3.0), (2, 2.0, 5.0))).toDF("id", "v1", "v2")

val sqlTrans = new SQLTransformer().setStatement(
  "SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__")

sqlTrans.transform(df).show()



v1.0
val mlsrc = ds.withColumn("label",
   when($"res_type" === "mx", 1).otherwise(
    when($"res_type" === "mn", 0).otherwise(2))).
     select(
      col("label").cast(DoubleType),
      col("ticker_id"),
      col("frmConfPeak")
     )


val seqDsForms :Seq[Dataset[Form]] = dsKeys.collect.toSeq.filter(elm => Seq(1,2,3,4,5,6).contains(elm.ticker_id)).map(elm => getFormsDb(elm.ticker_id,elm.bar_width_sec))

*/


--===========================================================================================================================================

spark-shell --driver-memory 3G --executor-memory 5G  --executor-cores 1 --jars "/opt/spark-2.3.2/jars/spark-cassandra-connector-assembly-2.3.2.jar" --conf "spark.cassandra.connection.host=192.168.122.192"

import org.apache.spark.sql._
import org.apache.spark.sql.functions.round
import org.apache.spark.sql.cassandra._
import com.datastax.spark.connector._
import org.apache.spark.sql.types.{DoubleType}
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.linalg.{DenseVector,SparseVector}
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import spark.implicits._

case class FormKeys(ticker_id     :Int,
                    bar_width_sec :Int)

case class Form(ticker_id     :Int,
                bar_width_sec :Int,
                ts_begin      :Long,
                ts_end        :Long,
                log_oe        :Double,
                res_type      :String,
                formdeepkoef  :Int,
                frmConfPeak   :Int)

case class ccLibSvm(label    :Int,
                    features :Int)

 def getKeysFormsDb(BarWidthSec: Int) :Dataset[FormKeys] = {
            import org.apache.spark.sql.functions._
            spark.read.format("org.apache.spark.sql.cassandra")
              .options(Map("table" -> "bars_forms", "keyspace" -> "mts_bars"))
              .load()
              .where(col("bar_width_sec") === BarWidthSec)
              .select(
                col("ticker_id"),
                col("bar_width_sec")
                ).distinct.sort(asc("ticker_id"),asc("bar_width_sec")).as[FormKeys]}

val dsKeys = getKeysFormsDb(600);
dsKeys.show()

 def getFormsDb(TickerID :Int, BarWidthSec: Int) :Dataset[Form] = {
            import org.apache.spark.sql.functions._
            spark.read.format("org.apache.spark.sql.cassandra")
              .options(Map("table" -> "bars_forms", "keyspace" -> "mts_bars"))
              .load()
              .where(col("ticker_id") === TickerID)
              .where(col("bar_width_sec") === BarWidthSec)
              .select(
                col("ticker_id"),
                col("bar_width_sec"),
                col("ts_begin"),
                col("ts_end"),
                col("log_oe"),
                col("res_type"),
                col("formdeepkoef"),
                col("formprops")("frmConfPeak").as("frmConfPeak").cast("Int")
                ).sort(asc("ticker_id"),asc("bar_width_sec"),asc("ts_begin")).as[Form]}

val seqDsForms :Seq[Dataset[Form]] = dsKeys.collect.toSeq.map(elm => getFormsDb(elm.ticker_id,elm.bar_width_sec))
val ds :Dataset[Form] = seqDsForms.reduce(_ union _)
ds.cache()
ds.count()

spark.catalog.dropGlobalTempView("frms")
ds.createGlobalTempView("frms")
spark.sql("""select res_type,frmConfPeak from global_temp.frms """).show()

val mlsrc = ds.withColumn("label",
   when($"res_type" === "mx", 1).otherwise(
    when($"res_type" === "mn", 2).otherwise(3))).
     select(
      col("label").cast(DoubleType),
      col("frmConfPeak")
     ).withColumn("f1", round(col("frmConfPeak")/100)).withColumn("f2", round((col("frmConfPeak") - round(col("frmConfPeak")/100)*100)/10)).withColumn("f3", col("frmConfPeak") % 10)

spark.catalog.dropGlobalTempView("tmp")
mlsrc.createGlobalTempView("tmp")

val clr =
spark.sql("""
select label,
       frmConfPeak,
       f1,
       f2,
       f3
  from(
select
       ds.label,
       ds.frmConfPeak,
       ds.f1,
       ds.f2,
       ds.f3,
       ds.cnt,
       row_number() over(partition by ds.frmConfPeak,ds.f1,ds.f2,ds.f3 order by ds.cnt desc) as rn
from (
select t.label,
       t.frmConfPeak,
       t.f1,
       t.f2,
       t.f3,
       sum(1) as cnt
  from global_temp.tmp t
 group by t.label,t.frmConfPeak,t.f1,t.f2,t.f3
 order by t.frmConfPeak,t.label ) ds
 ) where rn=1
""")

val assembler = new VectorAssembler().setInputCols(Array("f1","f2","f3" )).setOutputCol("features")
val dat = assembler.transform(clr)
val data = dat.drop("f1").drop("f2").drop("f3")
val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
val train = splits(0)
val test = splits(1)
println(" rows:"+data.count()+" train:"+train.count()+" test:"+test.count())

val layers = Array[Int](3, 5, 5, 3)
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setLabelCol("label").setFeaturesCol("features").setBlockSize(128).setSeed(1234L).setMaxIter(10)
val model = trainer.fit(train)
val result = model.transform(test)
val predictionAndLabels = result.select("prediction", "label")
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")





/*---------------------------------------------------------------------------------

scala> mlsrc.groupBy("label").count().show()
+-----+-----+
|label|count|
+-----+-----+
|  1.0|26030|
|  2.0|24483|
+-----+-----+

---------------------------------------------------------------------------------*/

val assembler = new VectorAssembler().setInputCols(Array("f1","f2","f3" )).setOutputCol("features")
val dat = assembler.transform(mlsrc)
val data = dat.drop("f1").drop("f2").drop("f3")
val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
val train = splits(0)
val test = splits(1)
println(" rows:"+data.count()+" train:"+train.count()+" test:"+test.count())

/*---------------------------------------------------------------------------------

scala> train.groupBy("label").count().show()
+-----+-----+
|label|count|
+-----+-----+
|  1.0|15639|
|  2.0|14595|
+-----+-----+

---------------------------------------------------------------------------------*/


val layers = Array[Int](3, 5, 5, 3)
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setLabelCol("label").setFeaturesCol("features").setBlockSize(128).setSeed(1234L).setMaxIter(10)
val model = trainer.fit(train)
val result = model.transform(test)
val predictionAndLabels = result.select("prediction", "label")
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")





//**************
// 0.5105281325509148
//**************

//recreate result from full source data.
val model = trainer.fit(data)
val result = model.transform(data)
result.show()

val predictionAndLabels = result.select("prediction", "label")
predictionAndLabels.show()
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
println(s"Full data set accuracy = ${evaluator.evaluate(predictionAndLabels)}")

//**************
// 0.5153128897511532
//**************

//save all

val vectorToColumn = udf{ (x:DenseVector, index: Int) => x(index) }

val resToCass = result.withColumn("p0",round(vectorToColumn(col("probability"),lit(0)),2)).withColumn("p1",round(vectorToColumn(col("probability"),lit(1)),2)).drop("probability").drop("rawPrediction").drop("features").withColumnRenamed("frmConfPeak", "frmconfpeak")

//.show()

resToCass.write.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "result", "keyspace" -> "mts_ml", "confirm.truncate" -> "true")).mode(org.apache.spark.sql.SaveMode.Overwrite).save()


&&&&&&&&&&&&&

val layers = Array[Int](3, 5, 5, 2)
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setLabelCol("label").setFeaturesCol("features").setBlockSize(128).setSeed(1234L).setMaxIter(100)
val model = trainer.fit(train)
val result = model.transform(test)
val predictionAndLabels = result.select("prediction", "label")
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")

3,5,5,2 100 -






///////////////  col("formprops")("frmConfPeak").as("frmConfPeak").cast("Int")
///////////////  val y = x.withColumn("col1", round($"col1", 3))

//.withColumn("prob",greatest(col("p0"),col("p1"))).drop("p0").drop("p1")

>> research
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import org.apache.spark.sql.functions._
result.write.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "result", "keyspace" -> "mts_ml")).save()



Test set accuracy = 0.5105281325509148

spark.sql("""
             select probability,
                    getprob(probability,1) as p1
               from global_temp.res
""").show()



spark.udf.register("getprob", (v: Vector[Double], i: Int) => v(i))
spark.catalog.listFunctions.filter('name like "%getprob%").show(false)


spark.catalog.dropGlobalTempView("res")
result.createGlobalTempView("res")

spark.sql("""
             select probability
               from global_temp.res
""").show()



For 1,2,3 - Test set accuracy        = 0.5035748792270531
For 1,2,3,4,5,6 - Test set accuracy  = 0.5455140798952194  setMaxIter(1000)
For 1,2,3,4,5,6 - Test set accuracy  = 0.5455140798952194  setMaxIter(100)
For 1,2,3,4,5,6 - Test set accuracy  = 0.5455140798952194  setMaxIter(10)

-- проход сверху вниз через уровень
spark.sql("""
             select res_type,
                    count(*) as cnt
               from global_temp.frms
              where frmConfPeak=123
              group by res_type
""").show()
+--------+---+
|res_type|cnt|
+--------+---+
|      mn|400|0
|      mx|423|1
+--------+---+
-- на всех данных
+--------+----+
|res_type| cnt|
+--------+----+
|      mn|1603|0
|      mx|1639|1
+--------+----+



-- проход снизу вверху через урвоень
spark.sql("""
             select res_type,
                    count(*) as cnt
               from global_temp.frms
              where frmConfPeak=321
              group by res_type
""").show()
+--------+---+
|res_type|cnt|
+--------+---+
|      mn|411|0
|      mx|449|1
+--------+---+
-- на всех данных
+--------+----+
|res_type| cnt|
+--------+----+
|      mn|1819|0
|      mx|1892|1
+--------+----+


Using of trained model:
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.attribute._
import org.apache.spark.ml.feature.VectorAssembler

val pointM = sc.parallelize(Seq((1.0, 2.0, 3.0))).toDF("f1", "f2" ,"f3")
val assemblerp = new VectorAssembler().setInputCols(Array("f1", "f2", "f3")).setOutputCol("features")
val transformed = assemblerp.transform(pointM)
val resultPoint = model.transform(transformed)
resultPoint.show()

val pointM = sc.parallelize(Seq((3.0, 2.0, 1.0))).toDF("f1", "f2" ,"f3")
val assemblerp = new VectorAssembler().setInputCols(Array("f1", "f2", "f3")).setOutputCol("features")
val transformed = assemblerp.transform(pointM)
val resultPoint = model.transform(transformed)
resultPoint.show()


val pointM = sc.parallelize(Seq((2.0, 2.0, 3.0))).toDF("f1", "f2" ,"f3")
val assemblerp = new VectorAssembler().setInputCols(Array("f1", "f2", "f3")).setOutputCol("features")
val transformed = assemblerp.transform(pointM)
val resultPoint = model.transform(transformed)
resultPoint.show()


val pointM = sc.parallelize(Seq((3.0, 2.0, 3.0))).toDF("f1", "f2" ,"f3")
val assemblerp = new VectorAssembler().setInputCols(Array("f1", "f2", "f3")).setOutputCol("features")
val transformed = assemblerp.transform(pointM)
val resultPoint = model.transform(transformed)
resultPoint.show()


















--==============================================================================================================================================














//val assembler = new VectorAssembler().setInputCols(Array("ticker_id", "frmConfPeak" )).setOutputCol("features")
//only one column in features

val assembler = new VectorAssembler().setInputCols(Array("frmConfPeak" )).setOutputCol("features")
val data = assembler.transform(mlsrc).drop("ticker_id").drop("frmConfPeak")
val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
val train = splits(0)
val test = splits(1)
println(" rows:"+data.count()+" train:"+train.count()+" test:"+test.count())

/*
 specify layers for the neural network:
 1) input layer of size 4 (features),
 2) two intermediate of size 5 and 4
 3) and output of size 3 (classes)
val layers = Array[Int](4, 5, 4, 3)
*/

val layers = Array[Int](2, 5, 5, 2)
val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setLabelCol("label").setFeaturesCol("features").setBlockSize(128).setSeed(1234L).setMaxIter(100)
val model = trainer.fit(train)
val result = model.transform(test)
result.show()

val predictionAndLabels = result.select("prediction", "label")
predictionAndLabels.show()
val evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
println(s"Test set accuracy = ${evaluator.evaluate(predictionAndLabels)}")




















--==============================================================================================================================================






/*

Convert dataset rowis into CC type
val dat = mlsrc.map(r => ccLibSvm(r.getAs[Int]("label"),r.getAs[Int]("features")))

import org.apache.spark.sql.types.IntegerType
val df2 = df.withColumn("yearTmp", df.year.cast(IntegerType))
    .drop("year")
    .withColumnRenamed("yearTmp", "year")

val assembler = new VectorAssembler().setInputCols(Array("TIR", "UO", "ROC20" )).setOutputCol("FEATURES")
val vd = assembler.transform(dat)

scala> mlsrc.printSchema()
root
 |-- label: decimal(38,18) (nullable = true)
 |-- features: integer (nullable = true)



//next we can use frms in Spark.sql

//#1 Query - count of forms by ticker
spark.sql("""
             select ticker_id,
                    count(*) as forms_cnt
               from global_temp.frms
               group by ticker_id
              order by 2 desc
""").show(50)

//#2 Query - count of rows by groups : frmConfPeak,res_type
spark.sql("""
             select frmConfPeak,res_type,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak,res_type
              order by 2,3 desc
""").show(50)

spark.sql("""
             select frmConfPeak,res_type,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak,res_type
              order by 1,2,3
""").show(50)

spark.sql("""
             select frmConfPeak,count(*) as forms_cnt
               from global_temp.frms
               group by frmConfPeak
              order by 2,1
""").show(50)

====================== CLUSTERING KMEANS ======================

//source data : metrics.

val ds_frm_1_600 = getFormsDb(1,600)
val ds_frm_3_600 = getFormsDb(3,600)

val dsFrms = ds_frm_1_600.union(ds_frm_3_600)

spark.catalog.dropGlobalTempView("fsrc")
dsFrms.createGlobalTempView("fsrc")

spark.sql("""
             select count(*) as cnt
               from global_temp.fsrc
""").show()

spark.sql("""
             select
                    ticker_id,
                    bar_width_sec,
                    ts_begin,
                    ts_end,
                    from_unixtime(ts_end/1000,'dd-MM-YYYY')  as ts_end_ddate,
                    (ts_end-ts_begin)/1000                   as ts_eb_secs
               from global_temp.fsrc
              order by ts_begin asc
""").show()

unuseful incorrect forms with small deep.

//convert it into scala collection
val ora_parts_list = df_ora_parts_names.as[(String)].collect.toSeq

for (ora_part_name <- ora_parts_list){
 println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
 println("        Processing partition:   "+ora_part_name)
 println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")

dsFrms.count()

df_forms_src.show()
df_forms_src.printSchema()
Dataset<Row> ds3 = ds1.union(ds2);


scala> df_forms_src.printSchema()
root
 |-- ticker_id: integer (nullable = true)
 |-- bar_width_sec: integer (nullable = true)
 |-- ts_begin: long (nullable = true)
 |-- ts_end: long (nullable = true)
 |-- log_oe: double (nullable = true)
 |-- res_type: string (nullable = true)
 |-- formdeepkoef: integer (nullable = true)
 |-- frmConfPeak: integer (nullable = true)

*/




























/*

DIFF

 val dat = df_tdata.filter(df_tdata("ddate") === "20180601").cache()
 val dat_norm_1 = dat.withColumn("svalTmp", dat("sval").cast(DoubleType)).drop("sval").withColumnRenamed("svalTmp","sval")


 case class rowData(ddate: Int,id_pok: Int,id_row: String, sval: BigDecimal)

 val df_tdata_ds = spark.read.format("org.apache.spark.sql.cassandra").options(Map("table"->"t_data","keyspace"->"msk_arm_lead")).load()
                   .filter($"ddate" === "20180601")
                   .withColumn("sval",$"sval".cast(DecimalType(38,18)))
                   .as[rowData]

val dat = spark.read.format("org.apache.spark.sql.cassandra").options(Map("table"->"t_data","keyspace"->"msk_arm_lead"))
.load().filter($"ddate" === "20180601").withColumn("sval",$"sval".cast(DecimalType(38,18))).as[rowData]

dat: org.apache.spark.sql.Dataset[rowData] = [ddate: int, id_pok: int ... 2 more fields]

case class rowNewData(ddate: Int,id_pok: Int,id_row: String, sval: BigDecimal, cntrlParam: Int)

val new_dat = dat.map { row =>(row: @unchecked) match {case rowData => rowNewData(row.ddate, row.id_pok, row.id_row, row.sval, {if (row.sval>65000.0) 1 else 0} )}}

*/


pipeline example

 val hiveContext = new HiveContext(sc)

    val df = hiveContext.sql("select * from prediction_test")
    df.show()
    val credit_indexer = new StringIndexer().setInputCol("transaction_credit_card").setOutputCol("creditCardIndex").fit(df)
    val category_indexer = new StringIndexer().setInputCol("transaction_category").setOutputCol("categoryIndex").fit(df)
    val location_flag_indexer = new StringIndexer().setInputCol("location_flag").setOutputCol("locationIndex").fit(df)
    val label_indexer = new StringIndexer().setInputCol("fraud").setOutputCol("label").fit(df)

    val assembler =  new VectorAssembler().setInputCols(Array("transaction_amount", "creditCardIndex","categoryIndex","locationIndex")).setOutputCol("features")
    val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)
    val pipeline = new Pipeline().setStages(Array(credit_indexer, category_indexer, location_flag_indexer, label_indexer, assembler, lr))

    val model = pipeline.fit(df)

    pipeline.save("/user/f42h/prediction/pipeline")
    model.save("/user/f42h/prediction/model")
 //   val sameModel = PipelineModel.load("/user/bob/prediction/model")
    model.transform(df).select("features","label","prediction")